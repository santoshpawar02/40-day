application imapct due to pod fail hence replicaset
isolating pod from replicaset by channging labels
Pod deletion cost - set the number with less number will deleted first, set by annotations
relpicaton controller  - will monitor replicaset to stop or start the Pod under replicaset
deployment - changin image version which did not worked in replicaset
deployment controller - take care of scaling replicaset
rolling update - rammped deployment
revision history
proportional scaling  - maxSurge - maxAvailables
canary deployments
environment variables in pods 
config maps
<!-- ----------------------------------------------------------------------- -->
<!--                                    x                                    -->
<!-- ----------------------------------------------------------------------- -->
<!-- ----------------------------------------------------------------------- -->
<!--                            Pod deletion cost                            -->
<!-- ----------------------------------------------------------------------- -->
The "Pod deletion cost" in Kubernetes, while not a direct monetary value, represents the **potential negative impact and risks associated with removing a specific Pod from your running application**. This impact varies significantly depending on the Pod's role, the application's architecture, and the resilience mechanisms you've implemented. A high deletion cost signifies a significant risk to availability or data integrity, while a low deletion cost indicates a more manageable removal.

**Factors Influencing Pod Deletion Cost (Recap with Slight Variation):**

1.  **Replica Count and Redundancy:**
    * **High Cost:** Last remaining replica of a critical, non-replicated service.
    * **Low Cost:** One of many healthy replicas in a well-scaled application.

2.  **Data Persistence and Statefulness:**
    * **High Cost:** Pod with non-replicated local storage containing critical, uncommitted data.
    * **Low Cost:** Pod in a stateless application where all data is stored externally.

3.  **Guaranteed Availability Requirements (PDBs):**
    * **High Cost (PDB Violation):** Deleting the Pod would violate the minimum availability defined in a Pod Disruption Budget.
    * **Low Cost (Within PDB):** Deleting the Pod respects the defined PDB.

4.  **Role in the Application Architecture:**
    * **High Cost:** Pod running a critical control plane component or a single instance of a vital backend service.
    * **Low Cost:** Pod handling transient tasks or part of a horizontally scalable worker pool.

5.  **Application's Handling of Termination Signals:**
    * **High Cost:** Application crashes or loses data upon receiving a `SIGTERM` signal.
    * **Low Cost:** Application gracefully shuts down, finishes ongoing tasks, and saves state before termination.

6.  **Startup Time and Warm-up:**
    * **Medium Cost:** Deleting a Pod that takes a significant time to start and become fully operational can lead to a temporary performance degradation while the replacement warms up.
    * **Low Cost:** Application instances start quickly and are immediately ready to serve.

**Real-Time Examples (Different Scenarios):**

1.  **High Deletion Cost - Leader Election in a Distributed System:**
    * **Scenario:** You have a distributed application where one Pod is elected as the leader to coordinate critical operations.
    * **Deletion Cost:** Very high. Forcefully deleting the leader Pod without a proper leader election mechanism in place can lead to split-brain scenarios, data inconsistencies, or temporary paralysis of the distributed system.

2.  **Low Deletion Cost - Stateless API Gateway:**
    * **Scenario:** You have multiple replicas of an API gateway Pod behind a load balancer. These gateways don't store any local state.
    * **Deletion Cost:** Low. Deleting one of these gateway Pods will likely go unnoticed by users as the load balancer will distribute traffic to the remaining healthy instances. Kubernetes will then spin up a replacement.

3.  **High Deletion Cost (PDB Violation) - ZooKeeper Ensemble:**
    * **Scenario:** You have a ZooKeeper ensemble (a distributed coordination service) with a PDB requiring a quorum of nodes to be available.
    * **Deletion Cost:** High. Deleting a Pod that would break the quorum (reduce the number of available nodes below the required minimum) will be prevented by the PDB to ensure the consistency and reliability of the ZooKeeper cluster.

4.  **Medium Deletion Cost - Caching Service with Data Locality:**
    * **Scenario:** You have a distributed caching service where some data is locally cached within each Pod for performance.
    * **Deletion Cost:** Medium. Deleting a cache Pod will lead to a temporary increase in latency as the replacement Pod needs to rebuild its cache. However, the overall application functionality remains.

5.  **Low Deletion Cost - Batch Processing Job Pod:**
    * **Scenario:** You have a Pod running a non-critical batch processing job that can be restarted or rerun without significant impact.
    * **Deletion Cost:** Low. If this Pod is deleted before completion (unless it's performing irreversible actions), the job can often be rescheduled or retried.

**Strategies to Mitigate High Pod Deletion Costs:**

* **Increase Replication:** Ensure sufficient redundancy for critical services.
* **Implement Leader Election:** For distributed systems, have robust leader election and failover mechanisms.
* **Utilize Persistent Volumes with Replication:** For stateful applications, ensure data is persisted externally and replicated across multiple instances.
* **Define and Enforce Pod Disruption Budgets (PDBs):** Protect critical workloads from accidental or excessive voluntary disruptions.
* **Implement Graceful Termination Hooks:** Allow applications time to shut down cleanly.
* **Design for Statelessness:** Minimize or eliminate local state within Pods.
* **Use StatefulSets for Ordered, Stable Stateful Workloads:** StatefulSets provide guarantees about pod identity and ordering during scaling and updates.

Understanding the "Pod deletion cost" is a crucial aspect of designing and operating resilient applications on Kubernetes. By considering the factors and implementing appropriate strategies, you can minimize the impact of both voluntary and involuntary disruptions.

[ ec2-user@ip-172-31-23-185 ~/learn-kubernetes ]$ kubectl patch pod rs-demo-v7b65 -p '{"metadata":{"annotations":{"controller.kubernetes.io/pod-deletion-cost":"100"}}}'
pod/rs-demo-v7b65 patched

54.237.197.169 | 172.31.23.185 | t2.micro | https://github.com/raghudevopsb83/learn-kubernetes.git
[ ec2-user@ip-172-31-23-185 ~/learn-kubernetes ]$ kubectl patch pod rs-demo-vzj6s -p '{"metadata":{"annotations":{"controller.kubernetes.io/pod-deletion-cost":"10"}}}'
pod/rs-demo-vzj6s patched

<!-- ----------------------------------------------------------------------- -->
<!--                          relpicaton controller                          -->
<!-- ----------------------------------------------------------------------- -->

A **Replication Controller** in Kubernetes is a legacy controller object that ensures a specified number of replica Pods are running at all times. It's very similar in purpose to a ReplicaSet. You define a desired number of Pods and a label selector, and the Replication Controller continuously monitors the running Pods that match the selector. If the actual number of running Pods deviates from the desired number, the Replication Controller takes action by creating or deleting Pods to match the specification.

**Key Characteristics:**

* **Ensures Desired Replicas:** You specify the exact number of Pod copies you want running.
* **Uses Selectors:** It uses a label selector to identify the Pods it manages.
* **Maintains Pod Count:** It actively monitors and adjusts the number of running Pods to match the desired state.
* **No Update Strategy:** Like ReplicaSets, Replication Controllers themselves do not handle rolling updates or rollbacks.
* **Superseded by ReplicaSets:** ReplicaSets are the newer and recommended replacement for Replication Controllers. ReplicaSets offer more expressive selector capabilities (using set-based selectors instead of only equality-based selectors).

**Think of a Replication Controller as an older version of a supervisor that keeps a specific number of identical application instances running.**

**Real-Time Examples:**

While ReplicaSets are preferred for new deployments, understanding Replication Controllers is still useful for maintaining older Kubernetes clusters or understanding historical context. Here are examples that would apply similarly to ReplicaSets:

1.  **Ensuring a Fixed Number of API Workers:**
    * **Scenario:** You have a critical API service, and you need to ensure that exactly three instances of the API worker Pods are always running to handle incoming requests reliably.
    * **Replication Controller Configuration:** You create a Replication Controller with `replicas: 3` and a selector that matches the labels of your API worker Pods (e.g., `app: api-worker`).
    * **Behavior:** If one of the API worker Pods crashes due to a software bug or node issue, the Replication Controller will detect that only two are running and will automatically create a new API worker Pod to bring the count back to three.

2.  **Maintaining a Set of Background Task Processors:**
    * **Scenario:** You have a set of background job processors that pull tasks from a queue. You want to ensure that four of these processor Pods are always active to keep up with the workload.
    * **Replication Controller Configuration:** You define a Replication Controller with `replicas: 4` and a selector that targets your background processor Pods (e.g., `role: background-processor`).
    * **Behavior:** If one of the background processor Pods is accidentally deleted by a user or fails, the Replication Controller will notice the reduced count and launch a new processor Pod to maintain the desired number of four.

3.  **Simple Manual Scaling (Similar to ReplicaSets):**
    * **Scenario:** You anticipate a temporary surge in traffic to a specific microservice and want to quickly increase its capacity.
    * **Scaling Up:** You can manually edit the `replicas` field in your existing Replication Controller's YAML file (using `kubectl edit rc <replication-controller-name>`) from, say, 2 to 5.
    * **Behavior:** The Replication Controller will observe the change in the desired replica count and create three new Pods that match its selector, effectively scaling up your microservice. You can later scale it back down in a similar way.

**Key Difference and Why ReplicaSets are Preferred:**

The main difference lies in the **selector capabilities**. Replication Controllers only support equality-based selectors (e.g., `app=my-app`). ReplicaSets, on the other hand, support set-based selectors (e.g., `app in (my-app, legacy-app)`, `environment notin (production)`). Set-based selectors offer much greater flexibility in selecting groups of Pods.

**In modern Kubernetes deployments, you should generally use ReplicaSets instead of Replication Controllers for managing the desired number of Pod replicas due to their more powerful selector capabilities.** Understanding Replication Controllers is primarily for working with older clusters or for historical context in learning Kubernetes. The core concept of ensuring a specific number of running Pods remains the same.

<!-- ----------------------------------------------------------------------- -->
<!--                               deployment                                -->
<!-- ----------------------------------------------------------------------- -->
A **Deployment** in Kubernetes is a higher-level controller that provides declarative updates for Pods and ReplicaSets. It allows you to describe the desired state for your application (e.g., the number of replicas, the container image version) and Kubernetes will work to achieve and maintain that state. Deployments manage the underlying ReplicaSets, enabling features like rolling updates and rollbacks with minimal downtime.

Think of a Deployment as a blueprint for your application's desired state and a manager that orchestrates the changes needed to reach and maintain that state, often using ReplicaSets under the hood.

**Key Characteristics:**

* **Declarative Updates:** You define the desired state in the Deployment specification, and Kubernetes handles the details of making it happen.
* **Rolling Updates:** Deployments can update your application to a new version incrementally, replacing old Pods with new ones without interrupting service.
* **Rollbacks:** If a new deployment fails or introduces issues, Deployments allow you to easily roll back to a previous stable version.
* **Scalability:** You can easily scale your application up or down by modifying the `replicas` field in the Deployment specification.
* **Manages ReplicaSets:** Deployments don't directly manage Pods; instead, they manage ReplicaSets, which in turn manage the Pods. This provides a layer of abstraction and enables the update and rollback functionalities.

**Real-Time Examples:**

1.  **Rolling out a New Version of a Web Application:**
    * **Scenario:** You have a web application running with a Deployment managing three replicas. You want to release a new version of the application with bug fixes and new features.
    * **Action:** You update the `spec.template.spec.containers[].image` field in your Deployment's YAML file to point to the new image version and apply the changes using `kubectl apply -f my-web-app-deployment.yaml`.
    * **Behavior:** The Deployment controller will create a new ReplicaSet with the new Pod template and gradually scale it up while scaling down the old ReplicaSet. By default, it will ensure that a certain number of Pods are always available during the update (rolling update strategy). Users experience minimal to no downtime during this process. If the new version has issues, you can easily roll back to the previous version using `kubectl rollout undo deployment/my-web-app-deployment`.

2.  **Scaling a Microservice Based on Traffic:**
    * **Scenario:** Your backend microservice is experiencing increased load due to higher user activity. You need to scale it out to handle the extra traffic.
    * **Action:** You can scale the Deployment managing your microservice by using the `kubectl scale` command:
        ```bash
        kubectl scale --replicas=7 deployment/my-backend-service
        ```
    * **Behavior:** The Deployment controller will update its desired replica count to 7. It will then instruct its underlying ReplicaSet to create the necessary additional Pods. Kubernetes will schedule these new Pods onto available nodes.

3.  **Rolling Back a Failed Deployment:**
    * **Scenario:** You deployed a new version of your application, but it's causing errors and impacting users. You need to quickly revert to the previous stable version.
    * **Action:** You can use the `kubectl rollout undo` command to roll back to the previous revision of your Deployment:
        ```bash
        kubectl rollout undo deployment/faulty-app-deployment
        ```
    * **Behavior:** The Deployment controller will identify the previous stable ReplicaSet and scale it up while scaling down the ReplicaSet of the faulty version. This effectively rolls back your application to the last known good state.

4.  **Pausing and Resuming a Deployment:**
    * **Scenario:** You need to temporarily halt a deployment in progress, perhaps to investigate an unexpected issue.
    * **Action:** You can pause the rollout of a Deployment:
        ```bash
        kubectl rollout pause deployment/in-progress-deployment
        ```
    * **Behavior:** The Deployment controller will stop the rolling update process in its current state. You can then resume the rollout later:
        ```bash
        kubectl rollout resume deployment/in-progress-deployment
        ```

**In essence, Deployments provide a robust and user-friendly way to manage the lifecycle of your applications in Kubernetes. They handle the complexities of updates, rollbacks, and scaling, allowing you to focus on defining the desired state of your application.** They are the standard and recommended way to manage stateless applications in Kubernetes.

The Deployment Controller is a core component within the Kubernetes control plane that is responsible for managing the state of Deployment objects. It watches for changes to Deployment specifications and works to ensure that the actual state of the running application (in terms of Pods managed by ReplicaSets) matches the desired state defined in the Deployment.

Think of the Deployment Controller as the brain behind the Deployment object. It's the process that takes your desired application configuration and orchestrates the necessary changes to the underlying ReplicaSets and Pods to achieve and maintain that configuration.

<!-- ----------------------------------------------------------------------- -->
<!--                   rolling update - rammped deployment                   -->
<!-- ----------------------------------------------------------------------- -->
**Rolling Update (Ramped Deployment)** is a deployment strategy in Kubernetes where new versions of your application are gradually rolled out by replacing old instances with new ones. This is done in a controlled manner, typically one or a few at a time, ensuring that the application remains available during the update process. The "ramped" aspect emphasizes the gradual and controlled nature of this replacement.

Think of a rolling update as carefully swapping out parts of a running system without bringing the whole thing down at once. It's like changing the tires on a moving car, one by one.

**Key Characteristics of Rolling Updates:**

* **Zero Downtime (or Minimal Downtime):** By replacing instances incrementally, the application usually remains available to users throughout the update process.
* **Controlled Rollout:** You can configure parameters like `maxSurge` (how many new Pods can be created above the desired count) and `maxUnavailable` (how many old Pods can be unavailable during the update) to control the pace and impact of the rollout.
* **Automatic Rollback on Failure:** Kubernetes can automatically roll back to the previous stable version if the new version encounters issues during the rollout (e.g., fails health checks).
* **Managed by Deployments:** Rolling updates are the default update strategy for Deployments in Kubernetes.

**How it Works (Simplified):**

1.  You update the Pod template in your Deployment specification (e.g., change the container image version).
2.  The Deployment Controller identifies the need for an update.
3.  It starts creating new Pods with the updated template. The `maxSurge` parameter controls how many new Pods can be created beyond the desired replica count.
4.  As new Pods become ready (pass their readiness probes), the Deployment Controller starts deleting old Pods. The `maxUnavailable` parameter controls how many old Pods can be unavailable at any given time.
5.  This process continues iteratively until all old Pods have been replaced by new ones.

**Real-Time Examples:**

1.  **Updating a Web Application:**
    * **Scenario:** You have a web application running with a Deployment of 5 replicas. You want to deploy a new version. Your Deployment has `maxSurge: 1` and `maxUnavailable: 1`.
    * **Action:** You update the container image in the Deployment YAML and apply it.
    * **Behavior:**
        * Kubernetes creates one new Pod with the new version (total of 6 Pods temporarily).
        * Once the new Pod is ready, Kubernetes deletes one old Pod (back to 5 Pods).
        * This repeats until all 5 old Pods are replaced by 5 new Pods. During this process, at least 4 Pods are always available.

2.  **Rolling Back a Faulty Application:**
    * **Scenario:** After a rolling update, you notice errors in the new version.
    * **Action:** You use `kubectl rollout undo deployment/my-app`.
    * **Behavior:** Kubernetes performs a rolling update, but this time it rolls back to the previous stable version. It gradually replaces the faulty new Pods with Pods running the previous version, again respecting `maxSurge` and `maxUnavailable`.

3.  **Deploying a New Backend Service:**
    * **Scenario:** You are deploying a new version of a backend API service managed by a Deployment with 3 replicas. You want a smooth transition for the clients consuming this API.
    * **Action:** You update the Deployment YAML with the new image and apply it.
    * **Behavior:** Kubernetes will gradually replace the old API server instances with the new ones. As new instances become ready, they start serving requests, and the old ones are terminated. This ensures that the API remains available throughout the update.

4.  **Scaling During a Rolling Update:**
    * **Scenario:** While a rolling update is in progress, you realize you need to increase the number of replicas due to increased load.
    * **Action:** You use `kubectl scale` to increase the `replicas` in the Deployment.
    * **Behavior:** Kubernetes will incorporate the scaling action into the rolling update process, ensuring that the final desired number of replicas of the new version is achieved.

**Benefits of Rolling Updates:**

* **Improved Availability:** Minimizes downtime during application updates.
* **Reduced Risk:** Allows for easier rollback if issues are found with the new version.
* **Gradual Exposure:** New versions can be exposed to a subset of traffic initially (though not the primary mechanism, it's a side effect of the gradual rollout).

**In summary, a rolling update (ramped deployment) is a crucial strategy for updating applications in Kubernetes with high availability. By gradually replacing old instances with new ones, it minimizes disruption to users and provides a safer way to deploy new versions of your software.** Deployments are the primary Kubernetes resource that orchestrates rolling updates.
<!-- ----------------------------------------------------------------------- -->
<!--                            Canary Deployment                            -->
<!-- ----------------------------------------------------------------------- -->
**Canary Deployment** is a deployment strategy in Kubernetes (and other software deployment contexts) where a new version of an application is rolled out to a small subset of users or infrastructure alongside the stable, production version. The new "canary" release serves a limited portion of the traffic, allowing you to test its performance, stability, and identify any potential issues in a real-world environment before fully rolling it out to all users.

Think of it like using a canary in a coal mine â€“ if the canary (the new version) shows signs of trouble, you can detect problems early and prevent them from affecting the entire system.

**Key Characteristics of Canary Deployments:**

* **Limited User Exposure:** Only a small percentage of users or requests are routed to the new version.
* **Real-World Testing:** Allows for testing the new version under actual production load and conditions.
* **Early Issue Detection:** Helps identify bugs, performance regressions, or infrastructure compatibility problems that might not have been caught in staging environments.
* **Controlled Rollout:** Provides a mechanism for a phased and cautious rollout.
* **Easy Rollback:** If issues are detected, you can quickly roll back to the stable version affecting only a small number of users.

**How it Works (Simplified in Kubernetes):**

1.  **Deploy the Canary Version:** You deploy the new version of your application alongside the existing stable version. This often involves creating a new Deployment with the new image and a small number of replicas.
2.  **Traffic Splitting:** You configure a Service or a traffic management layer (like Istio, Nginx Ingress with specific configurations, or a custom solution) to route a small percentage of incoming traffic to the canary Pods. The majority of traffic continues to go to the stable Pods.
3.  **Monitoring and Analysis:** You closely monitor the performance, error rates, and logs of the canary deployment. You compare its behavior to the stable version.
4.  **Decision Point:** Based on the monitoring data, you make a decision:
    * **Success:** If the canary performs well, you can gradually increase the traffic to the new version, eventually replacing the old one (effectively a rolling update).
    * **Failure:** If issues are detected, you can quickly roll back by scaling down or deleting the canary deployment and ensuring all traffic goes back to the stable version.

**Real-Time Examples:**

1.  **Testing a New Frontend UI:**
    * **Scenario:** You've developed a major redesign of your website's user interface. You want to test its usability and performance with real users before fully launching it.
    * **Implementation:** You deploy a new set of frontend Pods with the new UI. You configure your Ingress controller or a service mesh to route, say, 5% of incoming users to these new Pods based on a cookie, a specific user agent, or a weighted random distribution. The remaining 95% of users see the old UI.
    * **Analysis:** You monitor user behavior on the canary UI (e.g., bounce rates, conversion rates, page load times) and compare it to the stable version. If the metrics are positive, you gradually increase the traffic percentage to the new UI. If negative, you can quickly revert to the old UI for all users.

2.  **Evaluating a New Backend API Version:**
    * **Scenario:** You've refactored a critical backend API and want to ensure its stability and performance under production load with real client applications.
    * **Implementation:** You deploy a small number of Pods running the new API version. You configure your service mesh or a custom routing layer to send a small percentage of API requests (e.g., 10%) to these new Pods.
    * **Analysis:** You monitor the error rates, latency, and resource consumption of the canary API compared to the stable version. If any regressions are found, you can quickly direct all traffic back to the stable API.

3.  **Trialling a New Infrastructure Component (e.g., Database Driver):**
    * **Scenario:** You want to test the stability and performance of a new database driver in your production environment with a small subset of your application's traffic.
    * **Implementation:** You might deploy a specific set of application Pods configured to use the new driver, while the majority continue using the old one. You can use labels and service selectors to target these specific Pods.
    * **Analysis:** You monitor database performance and application stability for the Pods using the new driver. If issues arise, you can isolate the impact to the canary instances.

**Benefits of Canary Deployments:**

* **Reduced Blast Radius:** Limits the impact of a faulty new release to a small group of users.
* **Real-World Feedback:** Provides valuable insights into how the new version performs under actual production load.
* **Lower Risk:** Allows for a more cautious and controlled rollout.
* **Faster Issue Detection:** Problems are often identified earlier than with a full-scale rollout.
* **Easier Rollback:** Reverting to the stable version is typically quick and straightforward.

**Implementation Considerations:**

* **Traffic Splitting Mechanism:** Requires a robust and configurable traffic routing mechanism (Service Mesh, Ingress Controller, custom logic).
* **Monitoring and Alerting:** Comprehensive monitoring and alerting are crucial to effectively analyze the canary deployment.
* **Automated Analysis (Optional):** Advanced setups might involve automated analysis of metrics to detect anomalies in the canary release.
* **Session Management:** Careful consideration is needed for session persistence if users are being switched between the canary and stable versions.
* **Database Migrations:** Canary deployments can complicate database schema changes; careful planning is required.

In conclusion, canary deployments are a powerful strategy for reducing the risk associated with releasing new software versions in a production environment. By exposing the new version to a small, controlled subset of traffic, you can gain confidence in its stability and performance before a full rollout.


<!-- ----------------------------------------------------------------------- -->
<!--                      environment variables in pods                      -->
<!-- ----------------------------------------------------------------------- -->
**Environment variables** in Kubernetes Pods are dynamic name-value pairs that provide configuration information to the containers running within the Pod. They are a simple way to pass settings like database credentials, API keys, application-specific options, and service URLs to your applications without hardcoding them into the container image.

**Key Characteristics (Brief):**

* **Key-Value Pairs:** Simple name-value structure.
* **Dynamic Configuration:** Allow changing application behavior without rebuilding the image.
* **Accessible by Containers:** Containers within the Pod can easily read these variables.
* **Defined in Pod/Deployment Specs:** Configured in the `env` section of the container definition.

**Real-Time Examples (Brief):**

1.  **Database Connection Details:**
    ```yaml
    spec:
      containers:
        - name: my-app
          image: my-app:latest
          env:
            - name: DATABASE_HOST
              value: "mydb.example.com"
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: db-credentials
                  key: username
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-credentials
                  key: password
    ```
    * **Explanation:** The `my-app` container gets database host directly, while username and password are securely fetched from a Kubernetes Secret.

2.  **API Keys and Service URLs:**
    ```yaml
    spec:
      containers:
        - name: my-service
          image: my-service:latest
          env:
            - name: API_KEY
              valueFrom:
                configMapKeyRef:
                  name: api-config
                  key: primary_key
            - name: AUTH_SERVICE_URL
              value: "http://auth.internal"
    ```
    * **Explanation:** The `my-service` container retrieves an API key from a ConfigMap and gets the URL for an internal authentication service.

3.  **Application-Specific Settings:**
    ```yaml
    spec:
      containers:
        - name: worker
          image: worker-image:latest
          env:
            - name: MAX_RETRIES
              value: "5"
            - name: LOG_LEVEL
              value: "INFO"
    ```
    * **Explanation:** The `worker` container configures its maximum retry attempts and logging verbosity through environment variables.

In essence, environment variables provide a flexible and secure way to configure applications running in Kubernetes Pods, allowing for separation of configuration from the application image itself.

<!-- ----------------------------------------------------------------------- -->
<!--                               ConfigMaps                                -->
<!-- ----------------------------------------------------------------------- -->
**ConfigMaps** in Kubernetes are API objects used to store non-confidential configuration data in key-value pairs. They allow you to decouple configuration details from your container images, making your applications more portable and easier to manage. Containers within a Pod can consume configuration data from ConfigMaps as environment variables, command-line arguments, or as files in a volume.

**Key Characteristics (Brief):**

* **Key-Value Storage:** Stores configuration data in simple key-value pairs.
* **Non-Confidential Data:** Intended for non-sensitive configuration (use Secrets for sensitive data).
* **Multiple Consumption Methods:** Data can be accessed as environment variables, command-line arguments, or mounted as files.
* **Decouples Configuration:** Separates configuration from application code in container images.
* **Updatable:** Changes to ConfigMaps can be reflected in running Pods (with appropriate configuration).

**Real-Time Examples (Brief):**

1.  **Application Settings:**
    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: app-config
    data:
      DATABASE_URL: "mydb.example.com:5432"
      CACHE_SIZE: "128m"
      LOG_LEVEL: "INFO"
    ```
    * **Consumption:** Containers can read `DATABASE_URL`, `CACHE_SIZE`, and `LOG_LEVEL` as environment variables.

2.  **Web Server Configuration:**
    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: nginx-config
    data:
      nginx.conf: |
        server {
          listen 80;
          server_name example.com;
          location / {
            proxy_pass http://backend-service:8080;
          }
        }
    ```
    * **Consumption:** The `nginx.conf` content can be mounted as a file into the Nginx container.

3.  **Java Application Properties:**
    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: java-app-props
    data:
      application.properties: |
        server.port=8080
        spring.datasource.url=jdbc:postgresql://mydb:5432/appdb
        spring.redis.host=redis-master
    ```
    * **Consumption:** The `application.properties` file can be mounted into the Java application container, allowing Spring Boot to load its configuration.

4.  **Command-Line Arguments:**
    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: worker-args
    data:
      worker_flags: "--max-threads=10 --queue-name=priority-queue"
    ```
    * **Consumption:** The `worker_flags` value can be used to construct command-line arguments for the worker container.

In essence, ConfigMaps provide a structured way to manage and inject configuration data into your Kubernetes applications, promoting separation of concerns and simplifying updates.